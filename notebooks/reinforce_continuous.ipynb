{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LD_LIBRARY_PATH\"] =\"/home/daniel/anaconda3/envs/dev/lib/python3.11/site-packages/cv2/../../lib64:/home/daniel/.mujoco/mujoco210/bin:/usr/lib/nvidia\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import  IterableDataset,DataLoader\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torch.distributions.normal import Normal\n",
    "from utility import create_test_env, create_train_env, test_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f817c76f190>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "Display(visible=False, size=(1500, 1000)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALG_ID = 'reinforce_continuous'\n",
    "ENV_ID='InvertedDoublePendulum-v4'\n",
    "# ENV_ID='MountainCarContinuous-v0'\n",
    "VIDEO_DIR =os.path.join(ROOT_DIR,'videos',ALG_ID,ENV_ID)\n",
    "LOG_DIR = os.path.join(ROOT_DIR,'tboard',ALG_ID,ENV_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENVS=24\n",
    "ENTROPY_COEFF = 0.01\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "MAX_STEP = 10000\n",
    "MAX_EPOCHS = 101\n",
    "BATCH_SIZE = 1024\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, num_features, num_outputs,hidden_size=128) -> None:\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(in_features=num_features, out_features=hidden_size)\n",
    "        self.hidden = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.mu = nn.Linear(in_features=hidden_size, out_features=num_outputs)\n",
    "        self.sigma = nn.Linear(in_features=hidden_size, out_features=num_outputs)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x if torch.is_tensor(x) else torch.FloatTensor(x)\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.hidden(x))\n",
    "\n",
    "        mu = F.tanh(self.mu(x))\n",
    "        sigma = F.softplus(self.sigma(x)) + 0.001\n",
    "        return mu, sigma\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def pi(self,state):\n",
    "        p = state if torch.is_tensor(state) else torch.FloatTensor(state)\n",
    "        mu,sigma = self.forward(p)\n",
    "        actions = torch.normal(mu, sigma)\n",
    "        actions = actions.numpy()\n",
    "        return actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(IterableDataset):\n",
    "    def __init__(self,env,max_step,policy,discount_factor):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.max_step = max_step\n",
    "        self.policy = policy\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def __iter__(self):\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        returns = []\n",
    "        dones = []\n",
    "        state,_ = self.env.reset()\n",
    "        for step in range(self.max_step):\n",
    "            action = self.policy(state)\n",
    "            # obs, rews, terminateds, truncateds, infos\n",
    "            next_state,reward,done, truncated ,infos = self.env.step(action)\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            state = next_state\n",
    "\n",
    "        next_return =  np.zeros(self.env.unwrapped.num_envs)\n",
    "        for t in range(self.max_step-1,-1,-1):\n",
    "            reward = rewards[t]\n",
    "            return_ = reward + (1 - dones[t])*self.discount_factor*next_return\n",
    "            returns.insert(0,return_)\n",
    "            next_return = return_\n",
    "        \n",
    "        states =  np.concatenate(states, axis=0).astype(np.float32) \n",
    "        returns = np.concatenate(returns, axis=0).astype(np.float32) \n",
    "        actions = np.concatenate(actions, axis=0).astype(np.float32) \n",
    "\n",
    "        indices = np.arange(returns.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for i in indices:\n",
    "            yield states[i],actions[i],returns[i]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reinforce(L.LightningModule):\n",
    "    def __init__(self,env_id, num_envs,lr = 1e-3, entropy_coeff=0.01, hidden_size=64, discount_factor=0.99, max_step=100, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.env_id = env_id\n",
    "        self.train_env = create_train_env(env_name=env_id,num_envs=num_envs)\n",
    "        self.test_env = create_test_env(env_name=self.env_id, obs_rms=self.train_env.obs_rms, video_dir=VIDEO_DIR)\n",
    "        num_features = self.train_env.unwrapped.single_observation_space.shape[-1]\n",
    "        self.action_dims = self.train_env.unwrapped.single_action_space.shape[-1]\n",
    "        self.policy=Policy(num_features, self.action_dims,hidden_size=hidden_size)\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        state,action,returns = batch\n",
    "        \n",
    "        action = action.reshape(-1,self.action_dims)\n",
    "        returns = returns.reshape(-1,1)\n",
    "\n",
    "        mu, sigma = self.policy(state) \n",
    "        dist = Normal(mu, sigma)\n",
    "        log_prob = dist.log_prob(action).sum(dim=1, keepdim=True)\n",
    "\n",
    "        policy_loss = - log_prob * returns\n",
    "        entropy = dist.entropy().sum(dim=-1, keepdim=True)\n",
    "\n",
    "        loss = (policy_loss - self.hparams.entropy_coeff*entropy).mean()\n",
    "        self.log(\"episode/Policy Loss\", policy_loss.mean())\n",
    "        self.log(\"episode/Entropy\", entropy.mean())\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.test_env.obs_rms = self.train_env.obs_rms\n",
    "        average_return = test_agent(self.test_env, self.policy.pi, episodes=1, max_steps=MAX_STEP,video_dir=VIDEO_DIR)\n",
    "        self.log(\"episode/Average Return\", average_return)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_ds = MyDataset(env=self.train_env, discount_factor=self.hparams.discount_factor, max_step=self.hparams.max_step,policy=self.policy.pi,)\n",
    "        train_dl = DataLoader(train_ds, batch_size=self.hparams.batch_size)\n",
    "        return train_dl\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.policy.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/dev/lib/python3.11/site-packages/gymnasium/vector/__init__.py:53: UserWarning: \u001b[33mWARN: `gymnasium.vector.make(...)` is deprecated and will be replaced by `gymnasium.make_vec(...)` in v1.0\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/home/daniel/anaconda3/envs/dev/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/daniel/anaconda3/envs/dev/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.obs_rms to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.obs_rms` for environment variables or `env.get_wrapper_attr('obs_rms')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/daniel/anaconda3/envs/dev/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "reinforce = Reinforce(env_id=ENV_ID, \n",
    "                  lr=LR, \n",
    "                  num_envs=NUM_ENVS, \n",
    "                  discount_factor=DISCOUNT_FACTOR,\n",
    "                  batch_size=BATCH_SIZE, \n",
    "                  max_step=MAX_STEP,\n",
    "                  entropy_coeff=ENTROPY_COEFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reinforce(\n",
       "  (policy): Policy(\n",
       "    (input): Linear(in_features=11, out_features=64, bias=True)\n",
       "    (hidden): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (mu): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (sigma): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/daniel/anaconda3/envs/dev/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:176: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=2)`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator='cpu',\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    logger=TensorBoardLogger(save_dir=os.path.dirname(LOG_DIR), name=ENV_ID)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r $VIDEO_DIR\n",
    "!rm -r $LOG_DIR\n",
    "!mkdir -p $LOG_DIR\n",
    "!mkdir -p $VIDEO_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name   | Type   | Params\n",
      "----------------------------------\n",
      "0 | policy | Policy | 5.1 K \n",
      "----------------------------------\n",
      "5.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.1 K     Total params\n",
      "0.020     Total estimated model params size (MB)\n",
      "/home/daniel/anaconda3/envs/dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c77187286334aa48df61e8bcfcf08a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/dev/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.obs_rms to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.obs_rms` for environment variables or `env.get_wrapper_attr('obs_rms')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/dev/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.obs_rms to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.obs_rms` for environment variables or `env.get_wrapper_attr('obs_rms')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-10.mp4.\n",
      "Moviepy - Writing video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-10.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-10.mp4\n",
      "Moviepy - Building video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-20.mp4.\n",
      "Moviepy - Writing video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-20.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-20.mp4\n",
      "Moviepy - Building video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-30.mp4.\n",
      "Moviepy - Writing video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-30.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-30.mp4\n",
      "Moviepy - Building video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-40.mp4.\n",
      "Moviepy - Writing video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-40.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-40.mp4\n",
      "Moviepy - Building video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-50.mp4.\n",
      "Moviepy - Writing video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-50.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-50.mp4\n",
      "Moviepy - Building video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-60.mp4.\n",
      "Moviepy - Writing video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-60.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-60.mp4\n",
      "Moviepy - Building video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-70.mp4.\n",
      "Moviepy - Writing video /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-70.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/daniel/src/rl4fun/videos/reinforce_continuous/InvertedDoublePendulum-v4/rl-video-episode-70.mp4\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=reinforce,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
